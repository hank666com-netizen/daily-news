#!/usr/bin/env python3
"""
Explorer All-in-One v1.0 - æ•´åˆæ‰€æœ‰æ¢ç´¢ä»»åŠ¡
- GitHub AI Agents
- æ–°é—» + è‡ªå®šä¹‰è¯é¢˜
- ç¤¾äº¤åª’ä½“ (Reddit + X)
"""

import subprocess
import json
from datetime import datetime
import os
import re
from pathlib import Path

OUTPUT_DIR = os.path.expanduser("~/clawd/hank-second-brain/tech/exploration")
REPORT_FILE = f"{OUTPUT_DIR}/{datetime.now().strftime('%Y-%m-%d')}-explorer.md"
GITHUB_FILE = f"{OUTPUT_DIR}/{datetime.now().strftime('%Y-%m-%d')}-github-agents.md"
SOCIAL_FILE = f"{OUTPUT_DIR}/{datetime.now().strftime('%Y-%m-%d')}-social.md"
LEARNINGS_NEWS = f"{OUTPUT_DIR}/learnings.json"
LEARNINGS_SOCIAL = f"{OUTPUT_DIR}/learnings-social.json"

SEARCH_SCRIPT = os.path.expanduser("~/.agents/skills/search/scripts/search.sh")

def tavily_search(query, max_results=8, topic="news"):
    """Tavily æœç´¢"""
    cmd = [SEARCH_SCRIPT, json.dumps({
        "query": query,
        "topic": topic,
        "time_range": "day",
        "max_results": max_results,
        "include_raw_content": True
    })]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        if result.returncode == 0:
            return json.loads(result.stdout)
        return None
    except Exception as e:
        print(f"   âš ï¸ æœç´¢é”™è¯¯: {e}")
        return None

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if not text:
        return ""
    noise = ['provide news feedback', 'send a tip', 'limited-time offer',
             'subscribe to read', 'please enable js', "we've detected", 
             'test your news', 'election results']
    for n in noise:
        text = re.sub(n, '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s+', ' ', text).strip()
    return text[:300] + "..." if len(text) > 300 else text

def explore_github():
    """æ¢ç´¢ GitHub AI Agents"""
    print(f"\nğŸ“¦ === GitHub AI Agents æ¢ç´¢ ===")
    
    topics = [
        ("AI Agents", "site:github.com trending AI agent autonomous"),
        ("Langflow/RAG", "site:github.com Langflow RAG multi-agent"),
        ("Self-improving", "site:github.com self-improving autonomous agent"),
        ("OpenClaw", "site:github.com OpenClaw agent automation"),
        ("Memory Systems", "site:github.com vector database semantic memory RAG"),
    ]
    
    results = {}
    for name, query in topics:
        print(f"ğŸ” {name}...")
        data = tavily_search(query)
        items = []
        if data and 'results' in data:
            for r in data['results'][:8]:
                title = r.get('title', '')
                content = clean_text(r.get('content', '') or r.get('snippet', ''))
                if title and len(title) > 10:
                    items.append({'title': title.strip(), 'content': content})
        results[name] = items
        print(f"   â†’ {len(items)} æ¡")
    
    # ç”Ÿæˆ GitHub æŠ¥å‘Š
    report = f"# GitHub AI Agents æ¢ç´¢æŠ¥å‘Š | {datetime.now().strftime('%Y-%m-%d')}\n\n"
    report += f"**æ¢ç´¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"
    
    for name, items in results.items():
        report += f"## ğŸ”— {name}\n\n"
        for i, item in enumerate(items[:10], 1):
            report += f"{i}. **{item['title']}**\n"
            if item.get('content'):
                report += f"   ğŸ“ {item['content']}\n"
            report += "\n"
    
    with open(GITHUB_FILE, 'w') as f:
        f.write(report)
    
    print(f"\nâœ… GitHub æŠ¥å‘Š: {GITHUB_FILE}")
    return results

def explore_news():
    """æ¢ç´¢æ–°é—» + è‡ªå®šä¹‰è¯é¢˜"""
    print(f"\nğŸ“° === æ–°é—»æ¢ç´¢ ===")
    
    # æ ‡å‡†æ¥æº
    standard_sources = [
        ("Reuters", "ğŸ‡¬ğŸ‡§", "site:reuters.com latest news"),
        ("Bloomberg", "ğŸ’°", "site:bloomberg.com latest"),
        ("WSJ", "ğŸ“ˆ", "site:wsj.com latest"),
        ("AP News", "ğŸ‡ºğŸ‡¸", "site:apnews.com latest"),
        ("The Economist", "ğŸ‡¬ğŸ‡§", "site:economist.com latest"),
        ("Product Hunt", "ğŸš€", "site:producthunt.com today"),
        ("GitHub Trending", "ğŸ’»", "site:github.com trending today"),
    ]
    
    # è‡ªå®šä¹‰è¯é¢˜
    custom_sources = [
        ("OpenClaw", "ğŸ¦", "OpenClaw agent automation workflow AI"),
        ("Claude Code", "ğŸ¤–", "Claude Code AI coding assistant Anthropic"),
        ("Silicon Valley", "ğŸ™ï¸", "Silicon Valley tech startups AI funding news"),
        ("Hacker News", "ğŸ¯", "site:news.ycombinator.com best"),
        ("DevNews", "ğŸ‘¨â€ğŸ’»", "software engineering programming developer news"),
    ]
    
    results = {}
    
    for name, emoji, query in standard_sources + custom_sources:
        category = f"{emoji} {name}"
        print(f"ğŸ” {category}...")
        data = tavily_search(query)
        items = []
        if data and 'results' in data:
            for r in data['results'][:5]:
                title = r.get('title', '')
                title = re.sub(r'\s*[-|]\s*(Reuters|Economist|News|Hacker News)$', '', title)
                title = re.sub(r'\.com$', '', title)
                content = clean_text(r.get('content', '') or r.get('snippet', ''))
                url = r.get('url', '')
                if title and len(title) > 10:
                    items.append({'title': title.strip(), 'content': content, 'url': url})
        results[category] = items
        print(f"   â†’ {len(items)} æ¡")
    
    return results

def explore_social():
    """æ¢ç´¢ç¤¾äº¤åª’ä½“"""
    print(f"\nğŸ¦ğŸ’¬ === ç¤¾äº¤åª’ä½“æ¢ç´¢ ===")
    
    sources = [
        ("Reddit r/MachineLearning", "ğŸ’¬", "site:reddit.com/r/MachineLearning artificial intelligence"),
        ("Reddit r/LocalLLaMA", "ğŸ’¬", "site:reddit.com/r/LocalLLaMA local models Ollama"),
        ("X AI Discussions", "ğŸ¦", "site:x.com OR site:twitter.com AI artificial intelligence"),
    ]
    
    results = {}
    for name, emoji, query in sources:
        category = f"{emoji} {name}"
        print(f"ğŸ” {category}...")
        data = tavily_search(query)
        items = []
        if data and 'results' in data:
            for r in data['results'][:8]:
                title = r.get('title', '')
                title = re.sub(r'\s*[-|]\s*(reddit|x|twitter)$', '', title)
                content = clean_text(r.get('content', '') or r.get('snippet', ''))
                url = r.get('url', '')
                if title and len(title) > 10:
                    items.append({'title': title.strip(), 'content': content, 'url': url})
        results[category] = items
        print(f"   â†’ {len(items)} æ¡")
    
    return results

def generate_combined_report(github, news, social):
    """ç”Ÿæˆæ•´åˆæŠ¥å‘Š"""
    today = datetime.now().strftime('%Y-%m-%d')
    
    # ç»Ÿè®¡
    total_github = sum(len(v) for v in github.values())
    total_news = sum(len(v) for v in news.values())
    total_social = sum(len(v) for v in social.values())
    total = total_github + total_news + total_social
    
    report = f"""# ğŸ“Š Explorer ä¸€ä½“åŒ–æ¢ç´¢æŠ¥å‘Š | {today}

## ğŸ“Š æ¦‚è§ˆ

- **æ¢ç´¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ç‰ˆæœ¬**: v1.0 (All-in-One)
- **æ•°æ®æ¥æº**: Tavily Search

---

## ğŸ“¦ GitHub AI Agents ({total_github} æ¡)

"""
    
    for name, items in github.items():
        if not items:
            continue
        report += f"### ğŸ”— {name}\n\n"
        for i, item in enumerate(items[:5], 1):
            report += f"{i}. **{item['title']}**\n"
            if item.get('content'):
                report += f"   ğŸ“ {item['content']}\n"
            report += "\n"
    
    report += f"""
---

## ğŸ“° æ–°é—» + è‡ªå®šä¹‰è¯é¢˜ ({total_news} æ¡)

"""
    
    # æ ‡å‡†æ¥æº
    report += "### ğŸ›ï¸ ä¸»æµåª’ä½“\n\n"
    for name in ["Reuters", "Bloomberg", "WSJ", "AP News", "The Economist"]:
        key = f"{'ğŸ‡¬ğŸ‡§' if name in ['Reuters', 'The Economist'] else 'ğŸ’°' if name == 'Bloomberg' else 'ğŸ‡ºğŸ‡¸' if name == 'AP News' else 'ğŸ“ˆ'} {name}"
        if key in news and news[key]:
            report += f"**{name}**\n"
            for i, item in enumerate(news[key][:3], 1):
                report += f"{i}. {item['title']}\n"
            report += "\n"
    
    report += "### ğŸ¯ è‡ªå®šä¹‰è¯é¢˜\n\n"
    for name in ["OpenClaw", "Claude Code", "Silicon Valley", "Hacker News", "DevNews"]:
        emoji = "ğŸ¦ğŸ¤–ğŸ™ï¸ğŸ¯ğŸ‘¨â€ğŸ’»"[["OpenClaw", "Claude Code", "Silicon Valley", "Hacker News", "DevNews"].index(name)]
        key = f"{emoji} {name}"
        if key in news and news[key]:
            report += f"**{name}**\n"
            for i, item in enumerate(news[key][:3], 1):
                report += f"{i}. {item['title']}\n"
            report += "\n"
    
    report += f"""
---

## ğŸ¦ğŸ’¬ ç¤¾äº¤åª’ä½“ ({total_social} æ¡)

"""
    
    for category, items in social.items():
        if not items:
            continue
        report += f"### {category}\n\n"
        for i, item in enumerate(items[:5], 1):
            report += f"{i}. **{item['title']}**\n"
            if item.get('url'):
                report += f"   ğŸ”— [é“¾æ¥]({item['url']})\n"
            report += "\n"
    
    report += f"""---

## ğŸ“ˆ ç»Ÿè®¡

| ç±»å‹ | æ•°é‡ |
|------|------|
| GitHub AI Agents | {total_github} æ¡ |
| æ–°é—» + è¯é¢˜ | {total_news} æ¡ |
| ç¤¾äº¤åª’ä½“ | {total_social} æ¡ |
| **æ€»è®¡** | **{total} æ¡** |

---

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ä¸‹æ¬¡æ¢ç´¢**: æ˜å¤© 08:30
"""
    
    with open(REPORT_FILE, 'w') as f:
        f.write(report)
    
    print(f"\nâœ… æ•´åˆæŠ¥å‘Š: {REPORT_FILE}")

def main():
    today = datetime.now().strftime('%Y-%m-%d')
    print(f"ğŸš€ [Explorer All-in-One v1.0] å¼€å§‹æ¢ç´¢... | {today}")
    
    # 1. GitHub æ¢ç´¢
    github_results = explore_github()
    
    # 2. æ–°é—»æ¢ç´¢
    news_results = explore_news()
    
    # 3. ç¤¾äº¤åª’ä½“æ¢ç´¢
    social_results = explore_social()
    
    # ç”Ÿæˆæ•´åˆæŠ¥å‘Š
    generate_combined_report(github_results, news_results, social_results)
    
    print(f"\n{'='*50}")
    print(f"âœ… æ¢ç´¢å®Œæˆï¼")
    print(f"ğŸ“¦ GitHub: {GITHUB_FILE}")
    print(f"ğŸ¦ğŸ’¬ Social: {SOCIAL_FILE}")
    print(f"ğŸ“Š æ•´åˆ: {REPORT_FILE}")
    print(f"{'='*50}")

if __name__ == "__main__":
    main()
