#!/usr/bin/env python3
"""
Explorer Social v1.0 - æ¢ç´¢ Reddit å’Œ X çš„çƒ­é—¨ AI è¯é¢˜
- Reddit: r/MachineLearning, r/ArtificialIntelligence, r/LocalLLaMA
- X: AI ç›¸å…³çƒ­é—¨è®¨è®º
"""

import subprocess
import json
from datetime import datetime
import os
import re
from pathlib import Path

OUTPUT_DIR = os.path.expanduser("~/clawd/hank-second-brain/tech/exploration")
REPORT_FILE = f"{OUTPUT_DIR}/{datetime.now().strftime('%Y-%m-%d')}-social.md"
LEARNINGS_FILE = f"{OUTPUT_DIR}/learnings-social.json"

SEARCH_SCRIPT = os.path.expanduser("~/.agents/skills/search/scripts/search.sh")

def load_learnings():
    """åŠ è½½å­¦ä¹ æ•°æ®"""
    if os.path.exists(LEARNINGS_FILE):
        with open(LEARNINGS_FILE) as f:
            return json.load(f)
    return {
        "version": "1.0",
        "updated": datetime.now().isoformat(),
        "sources": {},
        "metrics": {"total_runs": 0, "avg_posts_per_run": 0}
    }

def save_learnings(data):
    """ä¿å­˜å­¦ä¹ æ•°æ®"""
    data["updated"] = datetime.now().isoformat()
    with open(LEARNINGS_FILE, 'w') as f:
        json.dump(data, f, indent=2)

def get_social_sources():
    """è·å–ç¤¾äº¤åª’ä½“æ¥æºåˆ—è¡¨"""
    return [
        {
            "name": "Reddit r/MachineLearning",
            "query": "site:reddit.com/r/MachineLearning artificial intelligence machine learning",
            "category": "ğŸ’¬ Reddit"
        },
        {
            "name": "Reddit r/ArtificialIntelligence", 
            "query": "site:reddit.com/r/ArtificialIntelligence AI LLMs agents",
            "category": "ğŸ’¬ Reddit"
        },
        {
            "name": "Reddit r/LocalLLaMA",
            "query": "site:reddit.com/r/LocalLLaMA local models Ollama LM Studio",
            "category": "ğŸ’¬ Reddit"
        },
        {
            "name": "X AI Discussions",
            "query": "site:x.com OR site:twitter.com AI artificial intelligence trending",
            "category": "ğŸ¦ X (Twitter)"
        },
        {
            "name": "X AI Researchers",
            "query": "site:x.com AI research GPT Claude models",
            "category": "ğŸ¦ X (Twitter)"
        }
    ]

def tavily_search(query, max_results=8):
    """ä½¿ç”¨ Tavily æœç´¢"""
    cmd = [SEARCH_SCRIPT, json.dumps({
        "query": query,
        "topic": "news",
        "time_range": "day",
        "max_results": max_results,
        "include_raw_content": True
    })]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        if result.returncode == 0:
            return json.loads(result.stdout)
        return None
    except Exception as e:
        print(f"   âš ï¸ æœç´¢é”™è¯¯: {e}")
        return None

def clean_social_text(text):
    """æ¸…ç†ç¤¾äº¤åª’ä½“æ–‡æœ¬"""
    if not text:
        return ""
    # ç§»é™¤å¸¸è§å™ªéŸ³
    noise = ['click to see', 'more:', 'read more', 'continue reading',
             'subscribe', 'sign up', 'log in', 'sign in']
    for n in noise:
        text = re.sub(n, '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s+', ' ', text).strip()
    return text[:300] + "..." if len(text) > 300 else text

def evaluate_source(name, posts_list, data):
    """è¯„ä¼°æ¥æºè´¨é‡"""
    if name not in data["sources"]:
        data["sources"][name] = {
            "success_rate": 0, "quality_score": 0,
            "attempts": 0, "successes": 0,
            "avg_posts": 0
        }
    
    source = data["sources"][name]
    source["attempts"] += 1
    
    if posts_list and len(posts_list) > 0:
        source["successes"] += 1
        source["avg_posts"] = len(posts_list)
        source["success_rate"] = source["successes"] / source["attempts"]
        # è´¨é‡åˆ†æ•°åŸºäºæˆåŠŸç‡å’Œå¸–å­æ•°é‡
        source["quality_score"] = min(source["success_rate"] * (len(posts_list) / 5), 1.0)
    
    return source

def main():
    today = datetime.now().strftime('%Y-%m-%d')
    print(f"ğŸ¦ğŸ’¬ [Explorer Social v1.0] æ¢ç´¢ Reddit å’Œ X çš„ AI è¯é¢˜...")
    
    # åŠ è½½å­¦ä¹ æ•°æ®
    learnings = load_learnings()
    learnings["metrics"]["total_runs"] += 1
    
    sources = get_social_sources()
    results = {}
    total_posts = 0
    
    for source_info in sources:
        name = source_info["name"]
        query = source_info["query"]
        category = source_info["category"]
        
        print(f"ğŸ” {name}...")
        
        data = tavily_search(query)
        posts_list = []
        
        if data and 'results' in data:
            for r in data['results'][:8]:
                title = r.get('title', '')
                content = clean_social_text(r.get('content', '') or r.get('snippet', ''))
                
                # æ¸…ç†æ ‡é¢˜ä¸­çš„ç«™ç‚¹å
                title = re.sub(r'\s*[-|]\s*(reddit|x|twitter|Reuters|News)$', '', title)
                title = re.sub(r'\|.*$', '', title)
                
                if title and len(title) > 10:
                    posts_list.append({
                        'title': title.strip(),
                        'content': content,
                        'url': r.get('url', '')
                    })
        
        results[category] = results.get(category, [])
        results[category].extend(posts_list)
        
        # è¯„ä¼°å¹¶å­¦ä¹ 
        evaluate_source(name, posts_list, learnings)
        
        print(f"   â†’ {len(posts_list)} æ¡")
        total_posts += len(posts_list)
    
    save_learnings(learnings)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# ç¤¾äº¤åª’ä½“ AI è¯é¢˜æ¢ç´¢æŠ¥å‘Š | {today}

## ğŸ“Š æ¦‚è§ˆ

- **æ¢ç´¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
- **æ•°æ®æ¥æº**: Reddit + X (Twitter) via Tavily Search
- **ç‰ˆæœ¬**: v1.0

---

"""
    
    category_emojis = {
        "ğŸ’¬ Reddit": "ğŸ’¬",
        "ğŸ¦ X (Twitter)": "ğŸ¦"
    }
    
    for category, posts in results.items():
        if not posts:
            continue
        emoji = category_emojis.get(category, "ğŸ“±")
        report += f"## {emoji} {category}\n\n"
        
        # å»é‡å¹¶æ˜¾ç¤ºå‰ 10 æ¡
        seen = set()
        for i, item in enumerate(posts[:10], 1):
            title = item['title']
            if title in seen:
                continue
            seen.add(title)
            
            report += f"{i}. **{title}**\n"
            if item.get('content'):
                report += f"   ğŸ“ {item['content']}\n"
            if item.get('url'):
                report += f"   ğŸ”— [é“¾æ¥]({item['url']})\n"
            report += "\n"
    
    avg_posts = learnings["metrics"]["avg_posts_per_run"]
    learnings["metrics"]["avg_posts_per_run"] = (avg_posts * (learnings["metrics"]["total_runs"] - 1) + total_posts) / learnings["metrics"]["total_runs"]
    save_learnings(learnings)
    
    report += f"""---

**æ€»å¸–å­æ•°**: {total_posts} æ¡
**æ¢ç´¢ç±»åˆ«**: {len(sources)} ä¸ª
**è¿è¡Œæ¬¡æ•°**: {learnings['metrics']['total_runs']}
"""
    
    with open(REPORT_FILE, 'w') as f:
        f.write(report)
    
    print(f"\nâœ… å®Œæˆï¼æŠ¥å‘Š: {REPORT_FILE}")
    print(f"ğŸ¦ğŸ’¬ å…±è·å– {total_posts} æ¡ç¤¾äº¤åª’ä½“å†…å®¹")

if __name__ == "__main__":
    main()
