#!/usr/bin/env python3
"""
Explorer News v4.0 - è‡ªæˆ‘å­¦ä¹ ç‰ˆæœ¬
- è¿½è¸ªæ¥æºè´¨é‡
- è‡ªåŠ¨ä¼˜åŒ–ç­–ç•¥
- å­¦ä¹ æˆåŠŸ/å¤±è´¥
- æŒç»­è¿›åŒ–
"""

import subprocess
import json
from datetime import datetime
import os
import re
from pathlib import Path

OUTPUT_DIR = os.path.expanduser("~/clawd/hank-second-brain/tech/exploration")
REPORT_FILE = f"{OUTPUT_DIR}/{datetime.now().strftime('%Y-%m-%d')}-news.md"
LEARNINGS_FILE = f"{OUTPUT_DIR}/learnings.json"

SEARCH_SCRIPT = os.path.expanduser("~/.agents/skills/search/scripts/search.sh")

def load_learnings():
    """åŠ è½½å­¦ä¹ æ•°æ®"""
    if os.path.exists(LEARNINGS_FILE):
        with open(LEARNINGS_FILE) as f:
            return json.load(f)
    return {
        "version": "1.0",
        "updated": datetime.now().isoformat(),
        "sources": {},
        "strategies": {
            "preferred_order": [],
            "blocked_sources": [],
            "auto_retry_failed": False
        },
        "metrics": {"total_runs": 0, "avg_news_per_run": 0, "avg_quality_score": 0}
    }

def save_learnings(data):
    """ä¿å­˜å­¦ä¹ æ•°æ®"""
    data["updated"] = datetime.now().isoformat()
    with open(LEARNINGS_FILE, 'w') as f:
        json.dump(data, f, indent=2)

def get_sources_from_learnings(learnings):
    """ä»å­¦ä¹ æ•°æ®ä¸­è·å–æ¥æºåˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰"""
    sources = {}
    blocked = learnings.get("strategies", {}).get("blocked_sources", [])
    
    for name, data in learnings.get("sources", {}).items():
        if name not in blocked:
            sources[name] = data
    
    # æŒ‰è´¨é‡åˆ†æ•°æ’åº
    sorted_sources = sorted(sources.items(), key=lambda x: x[1].get("quality_score", 0), reverse=True)
    return [name for name, _ in sorted_sources]

def get_query(name):
    """æ ¹æ®æ¥æºåç§°ç”ŸæˆæŸ¥è¯¢"""
    queries = {
        "Reuters": "site:reuters.com latest news",
        "The Economist": "site:economist.com latest",
        "Product Hunt": "site:producthunt.com today",
        "GitHub Trending": "site:github.com trending today",
        "AP News": "site:apnews.com latest",
        # è‡ªå®šä¹‰è¯é¢˜
        "OpenClaw": "OpenClaw agent automation AI",
        "Claude Code": "Claude Code AI coding assistant",
        "Silicon Valley": "Silicon Valley tech news startups",
        "Hacker News": "site:news.ycombinator.com front page",
        "DevNews": "software engineering programming news",
    }
    return queries.get(name, f"site:{name.lower()}.com latest news")

def get_custom_sources():
    """è·å–è‡ªå®šä¹‰è¯é¢˜æ¥æº"""
    return [
        {"name": "OpenClaw", "emoji": "ğŸ¦", "query": "OpenClaw agent automation workflow AI", "category": "ğŸ¤– OpenClaw"},
        {"name": "Claude Code", "emoji": "ğŸ¤–", "query": "Claude Code AI coding assistant Anthropic", "category": "ğŸ’» Claude Code"},
        {"name": "Silicon Valley", "emoji": "ğŸ™ï¸", "query": "Silicon Valley tech startups AI funding news", "category": "ğŸ™ï¸ ç¡…è°·æ–°é—»"},
        {"name": "Hacker News", "emoji": "ğŸ¯", "query": "site:news.ycombinator.com best", "category": "ğŸ¯ Hacker News"},
        {"name": "DevNews", "emoji": "ğŸ‘¨â€ğŸ’»", "query": "software engineering programming developer news", "category": "ğŸ‘¨â€ğŸ’» ç¨‹åºå‘˜æ–°é—»"},
    ]

def tavily_search(query, max_results=5):
    cmd = [SEARCH_SCRIPT, json.dumps({
        "query": query, "topic": "news",
        "time_range": "day", "max_results": max_results,
        "include_raw_content": True
    })]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        return json.loads(result.stdout) if result.returncode == 0 else None
    except:
        return None

def clean_text(text):
    if not text:
        return ""
    noise = ['provide news feedback', 'send a tip', 'limited-time offer',
             'subscribe to read', 'please enable js', "we've detected", 
             'test your news', 'election results']
    for n in noise:
        text = re.sub(n, '', text, flags=re.IGNORECASE)
    text = re.sub(r'\s+', ' ', text).strip()
    return text[:350] + "..." if len(text) > 350 else text

def evaluate_source(name, news_list, data):
    """è¯„ä¼°æ¥æºè´¨é‡å¹¶æ›´æ–°å­¦ä¹ æ•°æ®"""
    if name not in data["sources"]:
        data["sources"][name] = {
            "success_rate": 0, "quality_score": 0, "content_clean": False,
            "avg_content_length": 0, "attempts": 0, "successes": 0,
            "last_success": None, "failure_reason": None
        }
    
    source = data["sources"][name]
    source["attempts"] += 1
    
    if news_list:
        source["successes"] += 1
        source["last_success"] = datetime.now().strftime('%Y-%m-%d')
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        total_content = sum(len(n.get('content', '')) for n in news_list)
        avg_length = total_content / len(news_list)
        content_clean = all(len(n.get('content', '')) > 50 for n in news_list if n.get('content'))
        
        # æ›´æ–°åˆ†æ•°
        source["avg_content_length"] = avg_length
        source["content_clean"] = content_clean
        
        # è´¨é‡åˆ†æ•° = æˆåŠŸç‡ Ã— å†…å®¹è´¨é‡ Ã— é•¿åº¦å› å­
        length_factor = min(avg_length / 300, 1.0)
        quality_factor = 0.9 if content_clean else 0.6
        source["quality_score"] = 1.0 * quality_factor * length_factor
        source["success_rate"] = source["successes"] / source["attempts"]
    else:
        source["failure_reason"] = "no_content"
    
    return source

def optimize_strategy(data):
    """æ ¹æ®å­¦ä¹ ç»“æœä¼˜åŒ–ç­–ç•¥"""
    # æ›´æ–°ä¼˜å…ˆé¡ºåº
    sorted_sources = sorted(
        [(n, d.get("quality_score", 0)) for n, d in data["sources"].items()],
        key=lambda x: x[1], reverse=True
    )
    data["strategies"]["preferred_order"] = [n for n, _ in sorted_sources if n not in data["strategies"].get("blocked_sources", [])]
    
    # æ ‡è®°å®Œå…¨å¤±è´¥çš„æ¥æº
    blocked = []
    for name, source in data["sources"].items():
        if source.get("attempts", 0) >= 2 and source.get("success_rate", 0) == 0:
            blocked.append(name)
            source["failure_reason"] = "consistently_blocked"
    
    data["strategies"]["blocked_sources"] = blocked
    
    return data

def main():
    today = datetime.now().strftime('%Y-%m-%d')
    print(f"ğŸ“° [Explorer v4.0] è‡ªæˆ‘å­¦ä¹ æ¢ç´¢å¼€å§‹...")
    
    # åŠ è½½å­¦ä¹ æ•°æ®
    learnings = load_learnings()
    learnings["metrics"]["total_runs"] += 1
    
    # è·å–æ¥æºåˆ—è¡¨ï¼ˆæŒ‰å­¦ä¹ ä¼˜å…ˆçº§ï¼‰
    sources = get_sources_from_learnings(learnings)
    if not sources:
        sources = ["Reuters", "The Economist", "Product Hunt", "GitHub Trending", "AP News"]
    
    print(f"ğŸ§  å·²å­¦ä¹  {len(sources)} ä¸ªæ¥æºï¼ŒæŒ‰è´¨é‡æ’åº")
    
    results = {}
    total_news = 0
    
    # å¤„ç†æ ‡å‡†æ¥æº
    for name in sources:
        query = get_query(name)
        print(f"ğŸ” {name}...")
        
        data = tavily_search(query)
        news_list = []
        
        if data and 'results' in data:
            for r in data['results'][:5]:
                title = r.get('title', '').replace(' - Reuters', '').replace(' - The Economist', '')
                title = re.sub(r'\.com$', '', title)
                content = clean_text(r.get('content', '') or r.get('snippet', ''))
                if title and len(title) > 10:
                    news_list.append({'title': title.strip(), 'content': content})
        
        results[name] = news_list
        
        # è¯„ä¼°å¹¶å­¦ä¹ 
        evaluate_source(name, news_list, learnings)
        
        print(f"   â†’ {len(news_list)} æ¡")
        total_news += len(news_list)
    
    # å¤„ç†è‡ªå®šä¹‰è¯é¢˜æ¥æº
    print(f"\nğŸ” æ¢ç´¢è‡ªå®šä¹‰è¯é¢˜...")
    custom_sources = get_custom_sources()
    custom_results = {}
    
    for source_info in custom_sources:
        name = source_info["name"]
        query = source_info["query"]
        category = source_info["category"]
        emoji = source_info["emoji"]
        
        print(f"ğŸ” {category}...")
        
        data = tavily_search(query)
        news_list = []
        
        if data and 'results' in data:
            for r in data['results'][:5]:
                title = r.get('title', '')
                content = clean_text(r.get('content', '') or r.get('snippet', ''))
                # æ¸…ç†æ ‡é¢˜
                title = re.sub(r'\s*[-|]\s*(Reuters|Economist|News|Hacker News)$', '', title)
                title = re.sub(r'\.com$', '', title)
                title = re.sub(r'\|.*$', '', title)
                
                if title and len(title) > 10:
                    news_list.append({
                        'title': title.strip(),
                        'content': content,
                        'url': r.get('url', '')
                    })
        
        custom_results[category] = {
            'emoji': emoji,
            'news': news_list
        }
        
        print(f"   â†’ {len(news_list)} æ¡")
        total_news += len(news_list)
    
    # ä¼˜åŒ–ç­–ç•¥
    learnings = optimize_strategy(learnings)
    save_learnings(learnings)
    
    # æ›´æ–°æŒ‡æ ‡
    learnings["metrics"]["avg_news_per_run"] = (learnings["metrics"]["avg_news_per_run"] * (learnings["metrics"]["total_runs"] - 1) + total_news) / learnings["metrics"]["total_runs"]
    
    avg_quality = sum(d.get("quality_score", 0) for d in learnings["sources"].values()) / len(learnings["sources"]) if learnings["sources"] else 0
    learnings["metrics"]["avg_quality_score"] = avg_quality
    save_learnings(learnings)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# æ¯æ—¥æ–°é—»æ¢ç´¢æŠ¥å‘Š | {today}

## ğŸ“Š æ¦‚è§ˆ

- **æ¢ç´¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
- **æ•°æ®æ¥æº**: Tavily Search + Self-Learning + Custom Topics
- **è‡ªæˆ‘è¿›åŒ–**: v4.0 + Custom Topics
- **æ–°å¢è¯é¢˜**: OpenClaw, Claude Code, ç¡…è°·æ–°é—», ç¨‹åºå‘˜æ–°é—»

---

"""
    
    # æ ‡å‡†æ¥æº
    emojis = {"Reuters": "ğŸ‡¬ğŸ‡§", "The Economist": "ğŸ‡¬ğŸ‡§", "Product Hunt": "ğŸš€",
              "GitHub Trending": "ğŸ’»", "AP News": "ğŸ‡ºğŸ‡¸"}
    
    for name in sources:
        if name not in emojis:
            emojis[name] = "ğŸ“°"
    
    for name in sources:
        news = results.get(name, [])
        if not news:
            continue
        report += f"## {emojis.get(name, 'ğŸ“°')} {name}\n\n"
        for i, item in enumerate(news, 1):
            report += f"{i}. **{item['title']}**\n"
            if item.get('content'):
                report += f"   ğŸ“ {item['content']}\n"
            report += "\n"
    
    # è‡ªå®šä¹‰è¯é¢˜
    report += "\n---\n\n## ğŸ¯ è‡ªå®šä¹‰è¯é¢˜\n\n"
    
    for category, data in custom_results.items():
        if not data['news']:
            continue
        report += f"### {data['emoji']} {category}\n\n"
        for i, item in enumerate(data['news'][:5], 1):
            report += f"{i}. **{item['title']}**\n"
            if item.get('content'):
                report += f"   ğŸ“ {item['content']}\n"
            if item.get('url'):
                report += f"   ğŸ”— [é“¾æ¥]({item['url']})\n"
            report += "\n"
    
    report += f"""---

**æ€»æ–°é—»æ•°**: {total_news} æ¡ | **æ ‡å‡†æ¥æº**: {len(sources)} | **è‡ªå®šä¹‰è¯é¢˜**: {len(custom_sources)}
**å¹³å‡è´¨é‡**: {avg_quality:.2f} | **è¿è¡Œæ¬¡æ•°**: {learnings['metrics']['total_runs']}
**å­¦ä¹ æ—¶é—´**: {learnings['updated']}
**ä¸‹æ¬¡æ¢ç´¢**: æ˜å¤© 08:15
"""
    
    with open(REPORT_FILE, 'w') as f:
        f.write(report)
    
    print(f"\nâœ… å®Œæˆï¼æŠ¥å‘Š: {REPORT_FILE}")
    print(f"ğŸ§  å­¦ä¹ æ•°æ®å·²æ›´æ–°")

if __name__ == "__main__":
    main()
